# Анализ возможности применения многопоточности для запросов к LLM

## Текущее состояние

### Архитектура обработки
В текущей реализации запросы к LLM выполняются **последовательно**:

```python
# status_validator/main.py, строки 284-397
for start in range(0, len(entries), config.batch_size):
    batch = entries[start : start + config.batch_size]
    for entry in batch:
        # Последовательная обработка каждой записи
        result = validate_entry(entry, config, sheets_client, llm_client)
```

### Узкие места производительности

1. **Последовательные запросы к LLM**
   - Каждый запрос выполняется синхронно через `llm_client.generate()`
   - Время ожидания ответа от API (обычно 1-5 секунд на запрос)
   - При обработке 100 записей без кеша: ~100-500 секунд простого ожидания

2. **Синхронный OpenAI клиент**
   - Используется библиотека `openai` с синхронным API
   - Блокирующие HTTP-запросы
   - Таймаут: 60 секунд на запрос (настраивается)

3. **Batch обработка**
   - Размер батча: 10 записей (настраивается)
   - Внутри батча обработка последовательная
   - Батчи используются для промежуточной записи в Google Sheets

## Потенциал для оптимизации

### Теоретический прирост производительности

При обработке N записей без кеша:
- **Текущая производительность**: N × (среднее_время_запроса)
- **С многопоточностью**: N / workers × (среднее_время_запроса)
- **Ожидаемый прирост**: 3-10x при 3-10 потоках

### Примеры сценариев

| Записей | Текущее время | С 5 потоками | Прирост |
|---------|---------------|--------------|---------|
| 10      | 30 сек        | 6 сек        | 5x      |
| 50      | 150 сек       | 30 сек       | 5x      |
| 100     | 300 сек       | 60 сек       | 5x      |

## Варианты реализации многопоточности

### Вариант 1: ThreadPoolExecutor (рекомендуется)

**Преимущества:**
- ✅ Минимальные изменения в коде
- ✅ Работает с существующим синхронным OpenAI клиентом
- ✅ Простая реализация и отладка
- ✅ GIL не является проблемой (I/O-bound операции)

**Недостатки:**
- ⚠️ Больше памяти на потоки
- ⚠️ Ограничение максимального количества потоков

**Пример реализации:**
```python
from concurrent.futures import ThreadPoolExecutor, as_completed

def validate_batch_parallel(entries, config, sheets_client, llm_client, max_workers=5):
    results = []
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        future_to_entry = {
            executor.submit(validate_entry, entry, config, sheets_client, llm_client): entry
            for entry in entries
        }
        
        for future in as_completed(future_to_entry):
            entry = future_to_entry[future]
            try:
                result = future.result()
                results.append((entry, result))
            except Exception as exc:
                LOGGER.exception("Validation failed for row %s", entry.row_number)
                
    return results
```

### Вариант 2: asyncio + aiohttp

**Преимущества:**
- ✅ Максимальная производительность
- ✅ Эффективное использование ресурсов
- ✅ Масштабируется на сотни параллельных запросов

**Недостатки:**
- ❌ Требует полного переписывания LLM клиента
- ❌ Нужно заменить `openai` на асинхронную версию или использовать `aiohttp`
- ❌ Сложнее в отладке
- ❌ Требует изменений в `CacheStore` (SQLite + asyncio)

**Оценка трудозатрат:** 3-5 дней разработки

### Вариант 3: multiprocessing

**Не рекомендуется:**
- ❌ Избыточная сложность для I/O-bound задач
- ❌ Проблемы с передачей объектов между процессами
- ❌ Больше накладных расходов на создание процессов
- ❌ Сложности с SQLite кешем (не thread-safe между процессами)

## Ограничения и риски

### 1. Rate Limits API

**OpenAI API лимиты:**
- Requests per minute (RPM): 500-10,000 в зависимости от тарифа
- Tokens per minute (TPM): 10,000-2,000,000

**Рекомендации:**
- Начать с 3-5 параллельных потоков
- Добавить настройку `max_workers` в конфигурацию
- Реализовать обработку ошибок rate limiting (429)
- Использовать exponential backoff

### 2. Prompt Caching от OpenAI

**Важно:** Prompt caching лучше работает при последовательной обработке!

Из документации OpenAI:
- Кеш живёт 5-10 минут
- Экономия до 50% стоимости на закешированных токенах
- Лучше работает при последовательных запросах в течение короткого времени

**Влияние многопоточности:**
- ✅ При обработке в рамках одного батча (5-10 минут) кеш будет актуален
- ✅ Все запросы используют один `prompt_cache_key`
- ⚠️ При слишком большой параллельности может снизиться hit rate

**Рекомендация:** Ограничить количество параллельных потоков до 5-10

### 3. SQLite Cache (thread-safety)

**Текущая реализация:**
```python
# status_validator/cache.py
class CacheStore:
    def __init__(self, path: Path):
        self._conn = sqlite3.connect(str(path))  # Одно соединение на экземпляр
```

**Проблема:** SQLite соединение не является thread-safe по умолчанию

**Решения:**
1. **Использовать `check_same_thread=False`** (простое, но менее безопасное)
2. **Connection pool** - создавать соединение для каждого потока
3. **Lock/Semaphore** - синхронизировать доступ к кешу

### 4. Google Sheets API Limits

- Write requests per minute: 60
- Read requests per minute: 100

**Текущая реализация:** Периодическая запись батчами - не критично для многопоточности

## Рекомендуемый план внедрения

### Этап 1: Подготовка (1-2 часа)

1. Добавить настройку в конфиг:
```yaml
llm:
  max_workers: 5  # Количество параллельных потоков для запросов к LLM
```

2. Сделать SQLite cache thread-safe:
```python
class CacheStore:
    def __init__(self, path: Path) -> None:
        self._path = path
        self._lock = threading.Lock()  # Добавить блокировку
```

### Этап 2: Реализация ThreadPoolExecutor (2-4 часа)

1. Создать функцию параллельной валидации
2. Добавить обработку ошибок rate limiting
3. Реализовать exponential backoff
4. Сохранить порядок результатов

### Этап 3: Тестирование (2-3 часа)

1. Тесты с разным количеством потоков (1, 3, 5, 10)
2. Проверка корректности результатов
3. Измерение производительности
4. Тестирование обработки ошибок

### Этап 4: Документирование (1 час)

1. Обновить README.md
2. Добавить примеры в config.example.yaml
3. Документировать best practices

## Рекомендуемая конфигурация

```yaml
# config.yaml
llm:
  max_retries: 3
  max_workers: 5  # НОВЫЙ ПАРАМЕТР: количество параллельных потоков
  rate_limit_retry_delay: 1  # НОВЫЙ ПАРАМЕТР: задержка при rate limiting (секунды)
  providers:
    1:
      name: primary
      model_env: OPENAI_MODEL_1
      api_key_env: OPENAI_API_KEY_1
      temperature: 0.0
      max_output_tokens: 10000
      request_timeout: 60

batch_size: 10  # Оставляем для промежуточной записи в Sheets
```

## Альтернативные подходы

### Batch API от OpenAI (будущее)

OpenAI предлагает Batch API для асинхронной обработки:
- Отправка до 50,000 запросов одновременно
- Скидка 50% на стоимость
- Обработка в течение 24 часов

**Применимость:** Не подходит для real-time валидации, но может быть полезно для массовой обработки

## Метрики для мониторинга

После внедрения многопоточности следует отслеживать:

1. **Производительность:**
   - Время обработки на запись
   - Общее время выполнения
   - Количество записей в секунду

2. **Качество:**
   - Hit rate локального кеша
   - Hit rate prompt caching (из логов OpenAI)
   - Процент успешных запросов

3. **Ошибки:**
   - Rate limiting errors (429)
   - Timeout errors
   - Connection errors

4. **Стоимость:**
   - Количество токенов per request
   - Процент закешированных токенов
   - Общая стоимость

## Выводы

### Рекомендуется внедрить многопоточность через ThreadPoolExecutor

**Причины:**
1. ✅ Прирост производительности **3-5x** при минимальных изменениях
2. ✅ Низкий риск и простая реализация
3. ✅ Совместимость с prompt caching при правильной настройке
4. ✅ Общие трудозатраты: **5-10 часов**

**Оптимальные параметры:**
- `max_workers`: 5 (хороший баланс между скоростью и rate limits)
- `batch_size`: 10 (без изменений, для промежуточной записи)
- Добавить exponential backoff для rate limiting

**Не рекомендуется:**
- Multiprocessing (избыточная сложность)
- asyncio (слишком большие изменения на данном этапе)

**Следующие шаги:**
1. Прототип с ThreadPoolExecutor
2. Тестирование на реальных данных
3. Мониторинг метрик производительности и стоимости
4. Возможно, переход на asyncio в будущем для дополнительной оптимизации

